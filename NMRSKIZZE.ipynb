{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMRSKIZZE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaHvdNWOmS8/h1vssFLEKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DocBader/NumeraiAllstars/blob/main/NMRSKIZZE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBd2-qopsPK"
      },
      "source": [
        "**Step 1.Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNbNqUtPpy-n"
      },
      "source": [
        "# hier fügen wir nach und nach libs hinzu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-4-9Fm2pmTz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6eICq_aqDD-"
      },
      "source": [
        "**Step 2. Loading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAQN_s6VqH7t"
      },
      "source": [
        "df_train = pd.read_csv(\"numerai_training_data.csv\").set_index(\"id\")\n",
        "df_test = pd.read_csv('numerai_tournament_data.csv').set_index('id')\n",
        "\n",
        "#hier noch validation dataset und in diesem step kann man nach und nach ergänzen was für nen type of data man konkret bearbeiten will"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4uRhqgOqg2T"
      },
      "source": [
        "**Step 2.5 Easier written code for some tasks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2knmHtbsqzYn"
      },
      "source": [
        "feature_cols_to_keep = [] #welche features verwenden wir beim training\n",
        "eras_to_exclude_from_trainingset = []\n",
        "kept_eras_in_trainingset = []\n",
        "save_good_models  = True\n",
        "save_result_file = True\n",
        "cv_number = [] #anzahl an cross validations\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58IhiIY4q23Y"
      },
      "source": [
        "**Code for removing all the unnecessary columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgjcdhRCrF-I"
      },
      "source": [
        "no_feature_cols = ['id', 'era', 'data_type', 'target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uJB2sh1rG_M"
      },
      "source": [
        "**Step 3. Splitting Data into X and Y**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFHY60OzreFp"
      },
      "source": [
        "X sind wie in den meisten ML Projekten die Features und Y die Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNk_BH1ksSI8"
      },
      "source": [
        "X = df_train[features].values\n",
        "y = df_train.target.values\n",
        "features = [f for f in df_train if f.startswith('feature')]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRU8aU29sPv1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split ( X, y, test_size = 0.5, train_size = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDY2mVN0sWg3"
      },
      "source": [
        "Hier beginnt der Salat. Wir müssen herausfinden warum wenn wir das Dataset splitten, der GridSearchCV part nicht klappt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chdC2r0_sjGB"
      },
      "source": [
        "**Step 4. GridsearchCV for Parameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tjl1mFTseE1"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# hier beim GridSearch gehen wir erstmal die basic algos von sklearn durch. Für den Anfagn würde das reichen um sich ein BIld zu machen ob eins davon wenigsten ne etwas gute Accuracy hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hqx5JMssykq"
      },
      "source": [
        "model_params = {\n",
        "    'svm' :{\n",
        "        'model' : svm.SVC(gamma='auto'),\n",
        "        'params' : {\n",
        "            'C' : [1,10,20],\n",
        "            'kernel' : ['rbf', 'linear']\n",
        "        }\n",
        "\n",
        "\n",
        "    },\n",
        "\n",
        "    'random_forest' :{\n",
        "        'model' : RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [1,5,10]\n",
        "        }\n",
        "\n",
        "    },\n",
        "\n",
        "    'logistic_regression' : {\n",
        "        'model' : LogisticRegression(solver = 'liblinear', multi_class='auto'),\n",
        "        'params' :{\n",
        "            'C':[1,5,10]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'naive_bayes_gaussian' : {\n",
        "        'model' : GaussianNB(),\n",
        "        'params' :{}\n",
        "\n",
        "    },\n",
        "\n",
        "    'naive_bayes_multinomial' :{\n",
        "        'model' : MultinomialNB(),\n",
        "        'params' : {}\n",
        "    },\n",
        "\n",
        "    'decision_treee_classifier' :{\n",
        "        'model' : DecisionTreeClassifier(),\n",
        "        'params' : {\n",
        "            'criterion' : ['gini', 'entropy']\n",
        "        } \n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX6wT6wks3dW"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "scores = []\n",
        "\n",
        "for model_name, mp in model_params.items():\n",
        "  clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "  clf.fit(X_train,y_train)\n",
        "  scores.append({\n",
        "      'model': model_name,\n",
        "      'best_score' : clf.best_score_,\n",
        "      'best_params' : clf.best_params_\n",
        "  })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7mooHiJs7Oo"
      },
      "source": [
        "Der ganze GridSearch Code funktioniert in der Regel so bei den meisten anderen Projekten. Die Fehler meldung die bei mir grade kam war 'Input contains NaN, infinity or a value too large for dtype('float64').' das heisst erstmal daten halbieren oder verändern... evtl wirklich daten in hälfte teilen weil zu viel input..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H1Ji5WCtSSn"
      },
      "source": [
        "**Step 5. Creating Preliminary XGBOOST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq7sgFzmtLwv"
      },
      "source": [
        "model = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs= -1, colsample_bytree=0.1)\n",
        "\n",
        "\n",
        "model.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JON2ZCyQtVNd"
      },
      "source": [
        "Das XGBoost model können wir mithilfe der Ergebnisse im gridsearch verfeinern. also unsere mission ist erstmal vernfüfntig gridsearch auszuführen. Probier bitte aus evtl guck ob du andere Wege findest mit den Daten zu hantieren. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCY92woGtrLT"
      },
      "source": [
        "**Zusatz Optionen die erfolgreich sein könnten wenn wir die Daten richtig einsetzen : SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZpBlzxLt86m"
      },
      "source": [
        "**Preprocessing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTNqsavSt6__"
      },
      "source": [
        "from sklearn import preprocessing, pipeline\n",
        "\n",
        "preprocessor = pipeline.Pipeline(\n",
        "    [\n",
        "     ('ss', preprocessing.StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor.fit(X_training)\n",
        "\n",
        "X_training = preprocessor.transform(X_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "035ig29UuExp"
      },
      "source": [
        "\n",
        "X = df_train.drop(no_feature_cols, axis=1)\n",
        "\n",
        "\n",
        "y = df_train['target']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_training,y,test_size = 0.5, random_state=42)\n",
        "\n",
        "\n",
        "X_training = X.values #creating numpy arrays\n",
        "\n",
        "y_training = y.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ9fHScPuH6Z"
      },
      "source": [
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "    X_training,\n",
        "    y_training,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "\n",
        "from sklearn import ensemble\n",
        "\n",
        "model = ensemble.ExtraTreesClassifier(\n",
        "    n_estimators=10,\n",
        "    max_depth=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "X_train, X_test, y_train, y_test = model_selec"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}